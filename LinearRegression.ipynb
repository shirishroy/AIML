{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f670ac6a",
   "metadata": {},
   "source": [
    "The dataset includes the following features:\n",
    "\n",
    "Customer_ID: Unique identifier for each customer.\n",
    "Age: Age of the customer (includes missing values).\n",
    "Annual_Income: Annual income of the customer (includes missing values).\n",
    "Gender: Categorical column with \"Male\" and \"Female\".\n",
    "Electronics_Purchased: Categorical column with electronic items purchased (e.g., \"TV\", \"Laptop\").\n",
    "Monthly_Spend: Monthly spend on electronics (includes missing values).\n",
    "Sales: Number of units sold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e364a2a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 500, Number of columns: 7\n",
      "        Customer_ID         Age  Annual_Income Gender Electronics_Purchased  \\\n",
      "count    500.000000  450.000000     450.000000    500                   500   \n",
      "unique          NaN         NaN            NaN      2                     4   \n",
      "top             NaN         NaN            NaN   Male                Tablet   \n",
      "freq            NaN         NaN            NaN    257                   139   \n",
      "mean     250.500000   41.364444   69949.437778    NaN                   NaN   \n",
      "std      144.481833   13.373696   29539.304162    NaN                   NaN   \n",
      "min        1.000000   18.000000   20112.000000    NaN                   NaN   \n",
      "25%      125.750000   30.000000   42702.250000    NaN                   NaN   \n",
      "50%      250.500000   41.500000   70646.500000    NaN                   NaN   \n",
      "75%      375.250000   53.000000   97507.000000    NaN                   NaN   \n",
      "max      500.000000   64.000000  119575.000000    NaN                   NaN   \n",
      "\n",
      "        Monthly_Spend       Sales  \n",
      "count      450.000000  500.000000  \n",
      "unique            NaN         NaN  \n",
      "top               NaN         NaN  \n",
      "freq              NaN         NaN  \n",
      "mean      2479.532578    5.178000  \n",
      "std       1433.016057    2.558791  \n",
      "min        106.600000    1.000000  \n",
      "25%       1194.127500    3.000000  \n",
      "50%       2496.225000    5.000000  \n",
      "75%       3770.640000    7.000000  \n",
      "max       4988.350000    9.000000  \n",
      "Null values per column:\n",
      "Customer_ID               0\n",
      "Age                      50\n",
      "Annual_Income            50\n",
      "Gender                    0\n",
      "Electronics_Purchased     0\n",
      "Monthly_Spend            50\n",
      "Sales                     0\n",
      "dtype: int64\n",
      "Training set size: (400, 7), Test set size: (100, 7)\n",
      "Mean Squared Error (MSE): 7.12717649052704\n",
      "Mean Absolute Error (MAE): 2.3356674786842233\n",
      "R^2 Score: -0.09294083674948084\n",
      "\n",
      "Analysis:\n",
      "The model explains -9.29% of the variance in the target variable.\n",
      "Lower MSE and MAE indicate better model performance. Evaluate residuals for more insight.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 1: Importing the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Step 2: Loading the dataset using pandas\n",
    "file_path = \"Electronics_Sales_Dataset.csv\"  # Update the path if necessary\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Step 3: Display the number of columns and rows in the dataset\n",
    "print(f\"Number of rows: {df.shape[0]}, Number of columns: {df.shape[1]}\")\n",
    "\n",
    "# Step 4: Knowing the statistical data for each column\n",
    "print(df.describe(include='all'))\n",
    "\n",
    "# Step 5: Display how many null values each column has\n",
    "print(\"Null values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Step 6: Methods to replace null values\n",
    "# Fill numerical columns with their mean\n",
    "numerical_cols = [\"Age\", \"Annual_Income\", \"Monthly_Spend\"]\n",
    "for col in numerical_cols:\n",
    "    df[col].fillna(df[col].mean(), inplace=True)\n",
    "\n",
    "# Fill categorical columns with the mode\n",
    "categorical_cols = [\"Gender\", \"Electronics_Purchased\"]\n",
    "for col in categorical_cols:\n",
    "    df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "\n",
    "# Step 7: Encoding using one hot encoder or label encoder\n",
    "# Label Encoding for Gender\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"Gender\"] = label_encoder.fit_transform(df[\"Gender\"])  # Male: 1, Female: 0\n",
    "\n",
    "# One-Hot Encoding for Electronics_Purchased\n",
    "df = pd.get_dummies(df, columns=[\"Electronics_Purchased\"], drop_first=True)\n",
    "\n",
    "# Step 8: Scaling, normalization of the data\n",
    "scaler = StandardScaler()  # Standard scaling\n",
    "scaled_cols = [\"Age\", \"Annual_Income\", \"Monthly_Spend\"]\n",
    "df[scaled_cols] = scaler.fit_transform(df[scaled_cols])\n",
    "\n",
    "# Step 9: Importing the model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Step 10: Defining train test split\n",
    "X = df.drop(columns=[\"Customer_ID\", \"Sales\"])  # Features\n",
    "y = df[\"Sales\"]  # Target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 11: Defining X and Y variables\n",
    "print(f\"Training set size: {X_train.shape}, Test set size: {X_test.shape}\")\n",
    "\n",
    "# Step 12: Prediction for given values\n",
    "model.fit(X_train, y_train)  # Train the model\n",
    "y_pred = model.predict(X_test)  # Predict on the test set\n",
    "\n",
    "# Step 13: Performance evaluation\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"R^2 Score: {r2}\")\n",
    "\n",
    "# Step 14: Analysis of the results\n",
    "print(\"\\nAnalysis:\")\n",
    "print(f\"The model explains {r2 * 100:.2f}% of the variance in the target variable.\")\n",
    "print(\"Lower MSE and MAE indicate better model performance. Evaluate residuals for more insight.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa2e18d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cc13ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
